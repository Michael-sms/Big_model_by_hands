# 大语言模型学习项目（Build a Large Language Model (From Scratch)）

## 项目简介

本项目是基于《Build a Large Language Model (From Scratch)》一书的学习实践项目。原书由Sebastian Raschka编写，详细介绍了如何从零开始构建大型语言模型(LLM)。我通过实践书中的代码和概念，逐步掌握LLM的核心原理和实现方法。

**学习对象和引用出处**：
- 原书：[Build a Large Language Model (From Scratch)](https://www.manning.com/books/build-a-large-language-model-from-scratch)
- 官方代码库：[LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch)
- 中文翻译参考：[Build a Large Language Model (From Scratch) 中文版](https://github.com/skindhu/Build-A-Large-Language-Model-CN)

## 技术栈

- **编程语言**: Python 3.x
- **深度学习框架**: PyTorch 2.9.1
- **分词库**: tiktoken 0.12.0 (OpenAI BPE分词器)
- **开发环境**: Jupyter Notebook
- **数据处理**: 正则表达式、滑动窗口采样

## 当前学习进度

### 第2章：处理文本数据 ✅ 已完成
**具体实现内容**：
1. **分词器实现**
   - `SimpleTokenizerV1`: 基础分词器，基于正则表达式实现文本分词
   - `SimpleTokenizerV2`: 扩展版本，添加`<|unk|>`和`<|endoftext|>`特殊token处理未知词和文本边界
   
2. **BPE分词**
   - 使用tiktoken库实现字节对编码(BPE)分词
   - 支持GPT-2分词器配置
   
3. **数据预处理**
   - 滑动窗口数据采样方法
   - 创建输入-目标对用于下一个词预测任务
   - 上下文窗口大小可配置（默认4个token）

4. **训练数据**
   - 使用Edith Wharton的短篇小说《The Verdict》作为训练文本
   - 文件路径：`data/the-verdict.txt`
   - 经过BPE分词后得到5145个token

### 第3章：实现注意力机制 ✅ 已完成
**已实现内容**：
1. **自注意力机制**
   - `SelfAttention_v1`: 基础实现，使用`nn.Parameter`初始化权重矩阵
   - `SelfAttention_v2`: 改进版本，使用`nn.Linear`层实现，支持不同的权重初始化方式
   
2. **因果注意力机制**
   - 实现注意力掩码，防止模型访问序列中的后续信息
   - 使用`torch.tril`生成下三角掩码矩阵
   - 对掩码后的注意力权重进行重新归一化，确保每行和为1
   
3. **多头注意力机制**
   - 将注意力机制分解为多个"头"，每个头学习数据的不同方面
   - 使模型能够同时关注来自不同表示子空间的信息

### 第4章：从零开始实现一个用于文本生成的大语言模型 ✅ 已完成
**已实现内容**：
1. **Transformer核心架构**
   - 实现完整的Transformer编码器块
   - 包含多头注意力、前馈网络、层归一化和快捷连接

2. **多头注意力机制**
   - 完整实现`MultiHeadAttention`类
   - 支持可配置的头数和维度
   - 实现因果注意力掩码

3. **前馈神经网络**
   - 实现位置感知的前馈网络
   - 包含两层线性变换和激活函数

4. **层归一化和快捷连接**
   - 实现层归一化(Layer Normalization)
   - 添加快捷连接(Shortcut Connections)
   - 详细分析快捷连接在信息传递和梯度传播中的作用

### 第5章：在无标记数据集上进行预训练 ✅ 已完成
**已实现内容**：
1. **模型训练循环**
   - 实现完整的训练流程(train_model_simple函数)
   - 使用AdamW优化器及其参数配置
   - 训练过程中的定期评估(evaluate_model函数)

2. **训练评估**
   - 训练损失从9.781降到0.391
   - 验证损失从9.933降到6.452
   - 实现训练过程中的文本生成示例(generate_and_print_sample函数)

3. **优化器选择**
   - 对比Adam和AdamW优化器的差异
   - AdamW的正则化效果和优势
   - 学习率和权重衰减的参数设置


## 项目结构

```
.
├── ch02处理文本数据.ipynb      # 第2章：文本处理与分词实现
│   ├── SimpleTokenizerV1/V2实现
│   ├── BPE分词(tiktoken)
│   ├── 滑动窗口数据采样
│   └── 输入-目标对创建
├── ch03实现注意力机制.ipynb   # 第3章：注意力机制实现
│   ├── 自注意力机制(SelfAttention_v1/v2)
│   ├── 因果注意力机制
│   ├── 多头注意力机制
│   └── PyTorch实现细节
├── ch04从零开始实现一个用于文本生成的大语言模型.ipynb   # 第4章：Transformer实现
│   ├── Transformer核心架构
│   ├── 多头注意力实现(MultiHeadAttention)
│   ├── 前馈网络实现
│   ├── 层归一化和快捷连接
│   └── 完整Transformer模块
├── ch05在无标记数据集上进行预训练.ipynb   # 第5章：模型训练实现
│   ├── 训练循环实现(train_model_simple)
│   ├── AdamW优化器配置
│   ├── 训练评估方法
│   ├── 文本生成示例
│   └── 训练过程可视化
├── data/                      # 训练数据目录
│   └── the-verdict.txt        # 训练文本：Edith Wharton短篇小说
├── images/                    # 图表和示意图
│   ├── chapter2/             # 第2章相关图表
│   └── chapter3/             # 第3章相关图表
├── requirements.txt           # Python依赖包列表
└── README.md                  # 项目说明文档
```

## 数据使用

**训练数据**：
- **文件**: `data/the-verdict.txt`
- **内容**: Edith Wharton的短篇小说《The Verdict》
- **用途**: 用于分词器训练和注意力机制实验
- **处理**: 经过BPE分词后得到5145个token序列

**数据预处理流程**：
1. 读取原始文本文件
2. 使用tiktoken GPT-2分词器进行BPE分词
3. 应用滑动窗口采样创建训练样本
4. 生成输入-目标对用于语言模型训练

## 学习心得与收获

### 第2章收获
1. **文本预处理流程**：掌握了从原始文本到模型可处理token ID的完整流程，包括清洗、分词、编码等步骤。
2. **分词技术对比**：理解了简单规则分词与BPE统计分词的差异，BPE能更好地处理未登录词和稀有词。
3. **数据准备技巧**：学会了使用滑动窗口方法创建输入-目标对，这是训练语言模型的基础。
4. **特殊token设计**：理解了`<|unk|>`和`<|endoftext|>`等特殊token在语言模型中的作用。

### 第3章收获
1. **注意力机制原理**：深入理解了自注意力机制的计算过程，包括查询、键、值向量的作用。
2. **因果注意力实现**：掌握了如何通过掩码技术实现因果注意力，确保模型只能看到当前位置之前的信息。
3. **多头注意力优势**：理解了多头注意力如何让模型同时关注不同表示子空间的信息，提高模型表达能力。
4. **PyTorch实践**：通过实际代码实现，加深了对PyTorch张量操作和神经网络模块的理解。

### 第4章收获
1. **Transformer架构**：掌握了Transformer编码器的完整架构和实现细节。
2. **多头注意力**：深入理解了多头注意力机制的工作原理和实现方法。
3. **快捷连接**：认识到快捷连接在信息传递和梯度传播中的关键作用。
4. **模型构建**：通过完整实现Transformer模块，加深了对大语言模型架构的理解。

### 第5章收获
1. **训练流程**：掌握了完整的大语言模型训练流程，包括数据加载、前向传播、反向传播和参数更新。
2. **优化选择**：理解了AdamW优化器的优势及其参数设置对训练效果的影响。
3. **评估指标**：认识到训练损失和验证损失的差异及其反映的模型表现。
4. **训练可视化**：学会了通过生成示例文本直观评估模型的训练进展情况。

## 后续学习计划

1. **短期目标**：
   - 开始第6章"模型评估与部署"的学习
   - 深入了解模型评估指标
   - 探索模型部署方法

2. **中期目标**：
   - 完成第6章"模型评估与部署"的学习
   - 实现模型评估流程
   - 尝试简单的模型部署
   - 开始第7章"高级主题与应用"的学习

3. **长期目标**：
   - 完成完整GPT模型的构建和训练
   - 尝试在不同数据集和任务上的迁移学习
   - 探索模型优化和部署的实践

## 运行环境配置

1. 安装依赖：
   ```bash
   pip install -r requirements.txt
   ```

2. 主要依赖：
   - `torch==2.9.1`: PyTorch深度学习框架
   - `tiktoken==0.12.0`: OpenAI BPE分词器
   - `jupyter`: 笔记本环境

## 致谢

特别感谢原书作者Sebastian Raschka的精彩内容，以及中文翻译项目的贡献者，让我能够更轻松地学习这些知识。同时感谢开源社区提供的优秀工具和库，如PyTorch和tiktoken，让深度学习实践变得更加便捷。