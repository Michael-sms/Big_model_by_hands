{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "本章涵盖以下内容：\n",
    "\n",
    "+ **介绍不同的LLM微调方法**\n",
    "+ **准备用于文本分类任务的数据集**\n",
    "+ **调整预训练的 LLM 以便微调**\n",
    "+ **微调 LLM 以识别垃圾短信**\n",
    "+ **评估微调后的 LLM 分类器的准确性**\n",
    "+ **使用微调后的 LLM 对新数据进行分类**\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "- [6.1 不同类型的微调](#61-不同类型的微调)\n",
    "- [6.2 准备数据集](#62-准备数据集)\n",
    "- [6.3 创建数据加载器](#63-创建数据加载器)\n",
    "- [6.4 使用预训练权重初始化模型](#64-使用预训练权重初始化模型)\n",
    "- [6.5 添加分类头](#65-添加分类头)\n",
    "- [6.6 计算分类损失和准确率](#66-计算分类损失和准确率)\n",
    "- [6.7 使用监督数据对模型进行微调](#67-使用监督数据对模型进行微调)\n",
    "- [6.8 将 LLM 用于垃圾短信分类](#68-将-llm-用于垃圾短信分类)\n",
    "- [6.9 本章摘要](#69-本章摘要)\n",
    "\n",
    "\n",
    "-----"
   ],
   "id": "5773afdfc15011c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "在之前的章节中，我们实现了 LLM 的架构，进行了预训练，并学习了如何从外部来源（如 OpenAI）导入预训练权重。本章将在此基础上，通过微调 LLM 来完成特定目标任务，比如文本分类（见图 6.1）。我们将以一个具体的例子来说明如何将文本消息分类为垃圾短信或正常短信。\n",
    "\n",
    "<img src=\"../images/chapter6/figure6.1.png\" width=\"75%\" />\n",
    "\n",
    "图 6.1 展示了微调 LLM 的两种主要方式：用于分类的微调（步骤 8）和用于指令遵循的微调（步骤 9）。在下一节中，我们将深入探讨这两种微调方式。\n",
    "\n",
    "## 6.1 不同类型的微调\n",
    "\n",
    "微调语言模型最常见的方法是指令微调和分类微调。指令微调通过在一组任务上使用特定指令训练模型，用以提升模型对自然语言提示中任务描述的理解和执行能力，如图 6.2 所示。\n",
    "\n",
    "<img src=\"../images/chapter6/figure6.2.png\" width=\"75%\" />\n",
    "\n",
    "下一章将讨论指令微调，相关内容在图 6.2 中有所展示。而本章的重点是分类微调，如果您有机器学习基础，可能已经对这一概念比较熟悉。\n",
    "\n",
    "在分类微调中，模型被训练用来识别特定的一组类别标签，比如“垃圾短信”和“非垃圾短信”。分类任务的应用不仅限于 LLM 和电子邮件过滤，还包括从图像中识别不同种类的植物、将新闻分类到体育、政治或科技等主题，以及在医学影像中区分良性和恶性肿瘤。\n",
    "\n",
    "但有一个关键点需要注意，经过分类微调的模型只能预测训练中遇到的类别。例如，它可以判断某内容是‘垃圾短信’还是‘非垃圾短信’（如图 6.3 所示），但不能对输入文本提供其他方面的信息。\n",
    "\n",
    "<img src=\"../images/chapter6/figure6.3.png\" width=\"75%\" />\n",
    "\n",
    "与图6.3中所示的分类微调模型不同，指令微调模型通常可以执行更广泛的任务。分类微调模型可以视为高度专业化的模型，而相比之下，开发一个适用于各种任务的通用型模型通常更具挑战性。\n",
    "\n",
    "> [!NOTE]\n",
    ">\n",
    "> **选择合适的微调方式**\n",
    ">\n",
    "> 指令微调提升了模型基于用户指令进行理解和生成响应的能力。它适用于需要基于复杂用户指令处理多任务的模型，增强模型的灵活性和交互质量。而分类微调则适合需要将数据精确分类为预定义类别的任务，例如情感分析或垃圾短信检测。\n",
    ">\n",
    "> 虽然指令微调用途更广泛，但需要更大的数据集和更多的计算资源，才能训练出能胜任多种任务的模型。相比之下，分类微调所需的数据和计算量更少，但用途局限于模型已训练的特定类别。\n"
   ],
   "id": "c10ec9953542ca99"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6.2 准备数据集\n",
    "\n",
    "在本章的剩余部分，我们将对之前章节中实现并预训练的 GPT 模型进行修改和分类微调。我们从下载并准备数据集开始，如图 6.4 所示。\n",
    "\n",
    "<img src=\"../images/chapter6/figure6.4.png\" width=\"75%\" />\n",
    "\n",
    "为了提供一个直观实用的分类微调示例，我们将采用一个包含垃圾消息和非垃圾消息的文本消息数据集。\n",
    "\n",
    "注意，这里讨论的是通过手机发送的短信，而不是电子邮件。不过，相同的步骤也适用于电子邮件分类，感兴趣的读者可以在附录 B 的参考部分找到邮件垃圾分类数据集的链接。\n",
    "\n",
    "首先，通过以下代码下载数据集："
   ],
   "id": "b82973368178c2e1"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-14T13:20:58.094997500Z",
     "start_time": "2026-01-14T13:20:58.058511500Z"
    }
   },
   "source": [
    "# Listing 6.1 Downloading and unzipping the dataset\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from jinja2 import pass_context\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
    "    if data_file_path.exists():\n",
    "        print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
    "        return\n",
    "    with urllib.request.urlopen(url) as response:          #A\n",
    "        with open(zip_path, \"wb\") as out_file:\n",
    "            out_file.write(response.read())\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:        #B\n",
    "        zip_ref.extractall(extracted_path)\n",
    "\n",
    "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "    os.rename(original_file_path, data_file_path)          #C\n",
    "    print(f\"File downloaded and saved as {data_file_path}\")\n",
    "\n",
    "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
    "\n",
    "\n",
    "#A 下载数据集\n",
    "#B 解压数据集\n",
    "#C 为解压的数据集文件设置.csv文件扩展名"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sms_spam_collection\\SMSSpamCollection.tsv already exists. Skipping download and extraction.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "执行完上述代码后，数据集被保存为制表符分隔的文本文件“SMSSpamCollection.tsv”，位于“sms_spam_collection”文件夹中。我们可以将其加载到 pandas DataFrame 中，方法如下：",
   "id": "d728de6a5a1b96f9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T13:25:00.320599800Z",
     "start_time": "2026-01-14T13:24:59.435967100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(data_file_path, sep='\\t', header=None, names=[\"Label\", \"Text\"])\n",
    "df"
   ],
   "id": "d75f05a56f208309",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     Label                                               Text\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will ü b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "保存的数据集如图 6.5 所示：\n",
    "\n",
    "<img src=\"../images/chapter6/figure6.5.png\" width=\"75%\" />\n",
    "\n",
    "我们来看一下数据集中类别标签的分布情况："
   ],
   "id": "8362aeae146182d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T13:26:34.355784700Z",
     "start_time": "2026-01-14T13:26:34.310842600Z"
    }
   },
   "cell_type": "code",
   "source": "print(df[\"Label\"].value_counts())",
   "id": "9c28b8f21b136840",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "执行上述代码后，我们发现数据集中‘ham’（正常短信）比‘spam’（垃圾短信）出现频率更高：\n",
    "\n",
    "```\n",
    "Label\n",
    "ham 4825\n",
    "spam 747\n",
    "Name: count, dtype: int64\n",
    "```\n",
    "\n",
    "为了简化起见，同时也因为我们倾向于使用小数据集进行教学（这便于更快地微调 LLM），我们选择对数据集进行下采样，每个类别保留 747 个样本。尽管处理类别不平衡的方法有多种，但这超出了本书关于 LLM 的讨论范围。读者若有兴趣探索处理不平衡数据的方法，可以参考附录 B 的参考部分。\n",
    "\n",
    "我们可以通过以下代码对数据集进行下采样，以创建一个平衡的数据集："
   ],
   "id": "e744a579706ab1b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T13:28:10.538647Z",
     "start_time": "2026-01-14T13:28:10.478665500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Listing 6.2 Creating a balanced dataset\n",
    "def create_balanced_dataset(df):\n",
    "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]                                 #A\n",
    "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)      #B\n",
    "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])              #C\n",
    "    return balanced_df\n",
    "\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "print(balanced_df[\"Label\"].value_counts())\n",
    "\n",
    "\n",
    "#A 统计垃圾短信的实例数量\n",
    "#B 随机抽取正常邮件实例，使其数量与垃圾短信实例相同。\n",
    "#C 将正常短信子集与垃圾短信合并"
   ],
   "id": "300510f0f520635a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     747\n",
      "spam    747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "在执行了以上代码以平衡数据集后，我们可以看到现在垃圾短信和正常短信的数量相等。\n",
    "```\n",
    "Label\n",
    "ham 747\n",
    "spam 747\n",
    "Name: count, dtype: int64\n",
    "```\n",
    "接下来，我们将字符串类别标签 \"ham\" 和 \"spam\" 分别转换为整数类别标签 0 和 1："
   ],
   "id": "b7cf7fd1149676e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T13:30:48.862046100Z",
     "start_time": "2026-01-14T13:30:48.830103600Z"
    }
   },
   "cell_type": "code",
   "source": "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})",
   "id": "1746ecffd5cbec6",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "这个过程类似于将文本转换为 token ID，但与使用包含 5 万多个词的 GPT 词汇表不同，这里我们仅处理两个 token ID：0 和 1。\n",
    "\n",
    "我们还需创建一个`random_split`函数，将数据集划分为三部分：70%用于训练，10%用于验证，20%用于测试。这些比例是机器学习中用于训练、调整和评估模型的常见划分比例："
   ],
   "id": "1af7c20dba7c1e7b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T13:32:39.347436500Z",
     "start_time": "2026-01-14T13:32:39.326929700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Listing 6.3 Splitting the dataset\n",
    "def random_split(df, train_frac, validation_frac):\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)     #A\n",
    "\n",
    "    train_end = int(len(df) * train_frac)                               #B\n",
    "    validation_end = train_end + int(len(df) * validation_frac)\n",
    "\n",
    "    train_df = df[:train_end]                                           #C\n",
    "    validation_df = df[train_end:validation_end]\n",
    "    test_df = df[validation_end:]\n",
    "\n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)  #D\n",
    "\n",
    "\n",
    "#A 将整个 DataFrame 随机打乱\n",
    "#B 计算数据分割的索引\n",
    "#C 分割 DataFrame\n",
    "#D 测试集默认大小为 0.2（即剩余部分）"
   ],
   "id": "ec4a18753275a4de",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "此外，我们将数据集保存为 CSV 文件，以便后续复用：",
   "id": "9923a790fbcea679"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T13:37:29.521883800Z",
     "start_time": "2026-01-14T13:37:29.485369200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df.to_csv(\"train.csv\", index=None)\n",
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=None)"
   ],
   "id": "d4ddbff14e2b9577",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "本节中，我们已经完成了数据集的下载、数据平衡处理，并将其划分为训练集和验证集。在接下来的部分中，我们将设置用于模型训练的 PyTorch 数据加载器。",
   "id": "b952f2facbf10b3e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6.3 创建数据加载器\n",
    "\n",
    "在本节中，我们将开发 PyTorch 数据加载器，其概念与第 2 章中实现的加载器类似。\n",
    "\n",
    "在第2章中，我们使用滑动窗口技术生成了大小一致的文本块，并将它们分组成批次，以提高模型训练的效率。每个文本块都作为一个独立的训练实例。\n",
    "\n",
    "然而，本章中我们使用的垃圾短信数据集包含长度不一的文本消息。为了像第 2 章中的文本块那样对这些消息进行批处理，我们有两种处理方式：\n",
    "\n",
    "1. 将所有消息截断至数据集或批次中最短消息的长度。\n",
    "2. 将所有消息填充到数据集或批次中最长消息的长度。\n",
    "\n",
    "方案一的计算成本较低，但如果较短的消息远小于平均长度或最长消息长度，可能会导致显著的信息损失，从而降低模型的性能。因此，我们选择方案二，以完整保留所有消息的内容。\n",
    "\n",
    "为实现方案二，我们需要将所有消息填充到与数据集中最长消息相同的长度，对所有较短的消息添加填充 token。为此，我们使用 `\"<|endoftext|>\"` 作为填充 token，正如第 2 章中所讨论的。\n",
    "\n",
    "在实现细节上，我们可以在编码后的文本消息中添加与 `\"<|endoftext|>\"` 对应的 token ID，而不是直接将字符串 `\"<|endoftext|>\"` 附加到每条文本消息后，如图 6.6 所示。\n",
    "\n",
    "<img src=\"../images/chapter6/figure6.6.png\" width=\"75%\" />\n",
    "\n",
    "图 6.6 假定 50,256 是填充 token `<|endoftext|>` 的 token ID。我们可以通过使用 tiktoken 包中的 GPT-2 分词器对 `<|endoftext|>` 进行编码来进一步验证此 token ID 是否正确（该分词器在前几章中已使用过）:\n"
   ],
   "id": "52bfa3d1fdd17db1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T14:00:39.049792600Z",
     "start_time": "2026-01-14T13:59:47.638005300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ],
   "id": "30e92f7352515888",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "执行以上代码，我们发现确实返回了 [50256]。\n",
    "\n",
    "接着，我们需要实例化数据加载器。但在此之前，我们首先需要实现一个 PyTorch Dataset，用于定义数据的加载和处理方式。\n",
    "\n",
    "为此，我们定义了`SpamDataset`类，实现了图 6.6 中展示的概念。该类负责处理多个关键任务：它识别训练数据集中最长的序列，对文本消息进行编码，并确保通过填充 token 将其他序列补齐到与最长序列相同的长度。"
   ],
   "id": "32b7dc2a095120f3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T14:03:08.412169500Z",
     "start_time": "2026-01-14T14:03:03.267809500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Listing 6.4 Setting up a Pytorch Dataset class\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "        self.encoded_texts = [                                      #A\n",
    "            tokenizer.encode(text) for text in self.data[\"Text\"]\n",
    "        ]\n",
    "\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "\n",
    "            self.encoded_texts = [                                  #B\n",
    "                encoded_text[:self.max_length]\n",
    "                for encoded_text in self.encoded_texts\n",
    "            ]\n",
    "\n",
    "        self.encoded_texts = [                                      #C\n",
    "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
    "            for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"Label\"]\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _longest_encoded_length(self):\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_texts:\n",
    "            encoded_length = len(encoded_text)\n",
    "            if encoded_length > max_length:\n",
    "                max_length = encoded_length\n",
    "        return max_length\n",
    "\n",
    "\n",
    "#A 对文本进行预分词\n",
    "#B 若序列超过最大长度则进行截断\n",
    "#C 将序列填充至最长序列长度"
   ],
   "id": "baa7facd0e8e2be6",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "`SpamDataset`类从之前创建的 CSV 文件中加载数据，使用 tiktoken 库中的 GPT-2 分词器对文本进行分词，并支持将序列填充或截断为统一长度（由最长序列或预定义的最大长度决定）。这样可以确保每个输入张量大小一致，从而满足接下来数据加载器创建批量训练数据的需求：",
   "id": "3ffc05ea189ff754"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T14:05:17.336178900Z",
     "start_time": "2026-01-14T14:05:17.255917700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = SpamDataset(\n",
    "    csv_file=\"train.csv\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=None\n",
    ")"
   ],
   "id": "f3e80043e09633d3",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "请注意，数据集的 `max_length` 属性中存储了最大序列长度。如果想要查看最长序列的 token 数量，可以使用以下代码：",
   "id": "dbe19f50a357a9d2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T14:05:43.541688300Z",
     "start_time": "2026-01-14T14:05:43.488731500Z"
    }
   },
   "cell_type": "code",
   "source": "print(train_dataset.max_length)",
   "id": "66b3768d4cf0f501",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "代码输出了 120，表明最长的序列不超过 120 个 token，这也是文本消息的常见长度。值得注意的是，我们之前预训练的模型的上下文长度限制为 1,024 个 token，因此可以处理最长 1,024 个 token 的序列。如果数据集中包含更长的文本，可以在创建训练数据集时传入 `max_length=1024` 参数，以确保数据不会超出模型支持的输入（上下文）长度。\n",
    "\n",
    "接下来，我们将验证集和测试集的序列填充到与训练集中最长序列相同的长度。需要注意的是，如果验证集和测试集中的某些样本长度超过了训练集中最长样本的长度，会在先前定义的 `SpamDataset` 代码中通过 `encoded_text[:self.max_length]` 进行截断。这种截断是可选的；如果确保验证集和测试集中没有超过 1,024 个 token 的序列，也可以将 `max_length` 设置为 `None` 来避免截断。\n"
   ],
   "id": "89e2d4c08d0760ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T14:07:03.226625800Z",
     "start_time": "2026-01-14T14:07:03.191132600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val_dataset = SpamDataset(\n",
    "    csv_file=\"validation.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "test_dataset = SpamDataset(\n",
    "    csv_file=\"test.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ],
   "id": "8232fec812e18081",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "将以上的数据集作为输入，我们就可以实例化数据加载器（可以回顾第 2 章中的操作）。然而，在本例中，目标表示的是类别标签，而非文本中的下一个 token。例如，选择批量大小为 8 时，每个批次包含 8 个长度为 120 的训练样本和相应的类别标签，如图 6.7 所示。\n",
    "\n",
    "<img src=\"../images/chapter6/figure6.7.png\" width=\"75%\" />\n",
    "\n",
    "以下代码创建了训练集、验证集和测试集的数据加载器，以批量大小为 8 加载文本消息及其标签（如图 6.7 所示）："
   ],
   "id": "969ff43e8455ee97"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T14:10:14.685862500Z",
     "start_time": "2026-01-14T14:10:14.643352400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Listing 6.5 Creating a Pytorch DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")"
   ],
   "id": "87a827620fbfafd2",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "为了确保数据加载器正常工作并确实返回了预期大小的批次数据，我们可以遍历训练集数据加载器，并打印最后一个批次的张量维度：",
   "id": "c84f5e4eac217163"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T14:14:05.348820Z",
     "start_time": "2026-01-14T14:14:05.265407300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions:\", target_batch.shape)"
   ],
   "id": "8db1f8c947ec5a90",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch dimensions: torch.Size([8, 120])\n",
      "Label batch dimensions: torch.Size([8])\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "如上所示，输入批次包含 8 个训练样本，每个样本包含 120 个token。标签张量存储了对应 8 个训练样本的类别标签。\n",
    "\n",
    "最后，为了了解数据集的大小，可以打印每个数据集的批次数："
   ],
   "id": "cf93c4061c018a9f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T14:14:23.504498800Z",
     "start_time": "2026-01-14T14:14:23.419110400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ],
   "id": "e2d3d30e87cbc0ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 training batches\n",
      "19 validation batches\n",
      "38 test batches\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "各数据集的批次数如下：\n",
    "```\n",
    "130 training batches\n",
    "19 validation batches\n",
    "38 test batches\n",
    "```\n",
    "本章的数据准备工作到此结束，接下来我们将初始化模型以准备进行微调。"
   ],
   "id": "9a58e9c87e0d6fa1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6.4 使用预训练权重初始化模型\n",
    "在本节中，我们将准备用于垃圾短信分类微调的模型。首先，我们初始化上一章使用过的预训练模型，如图 6.8 所示。\n",
    "\n",
    "<img src=\"../images/chapter6/figure6.8.png\" width=\"75%\" />\n",
    "\n",
    "现在我们通过复用第 5 章的配置，开始进行模型准备过程："
   ],
   "id": "f2f1876c23ada22c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T14:41:41.944511400Z",
     "start_time": "2026-01-14T14:41:41.908838200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257, # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"drop_rate\": 0.0, # Dropout rate\n",
    "    \"qkv_bias\": True # Query-key-value bias\n",
    "}\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "assert train_dataset.max_length <= BASE_CONFIG[\"context_length\"], (\n",
    "    f\"Dataset length {train_dataset.max_length} exceeds model's context \"\n",
    "    f\"length {BASE_CONFIG['context_length']}. Reinitialize data sets with \"\n",
    "    f\"`max_length={BASE_CONFIG['context_length']}`\"\n",
    ")"
   ],
   "id": "a6dc52390a57d04c",
   "outputs": [],
   "execution_count": 23
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
